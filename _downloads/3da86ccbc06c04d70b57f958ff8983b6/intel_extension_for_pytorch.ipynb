{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nIntel\u00ae Extension for PyTorch*\n*******************************\n**Author**: `Jing Xu <https://github.com/jingxu10>`_\n\nIntel Extension for PyTorch* extends PyTorch with optimizations for extra\nperformance boost on Intel hardware. Most of the optimizations will be\nincluded in stock PyTorch releases eventually, and the intention of the\nextension is to deliver up to date features and optimizations for PyTorch\non Intel hardware, examples include AVX-512 Vector Neural Network\nInstructions (AVX512 VNNI) and Intel\u00ae Advanced Matrix Extensions (Intel\u00ae AMX).\n\nIntel\u00ae Extension for PyTorch* has been released as an open\u2013source project\nat `Github <https://github.com/intel/intel-extension-for-pytorch>`_.\n\nFeatures\n--------\n\n* Ease-of-use Python API: Intel\u00ae Extension for PyTorch* provides simple\n  frontend Python APIs and utilities for users to get performance optimizations\n  such as graph optimization and operator optimization with minor code changes.\n  Typically, only 2 to 3 clauses are required to be added to the original code.\n* Channels Last: Comparing to the default NCHW memory format, channels_last\n  (NHWC) memory format could further accelerate convolutional neural networks.\n  In Intel\u00ae Extension for PyTorch*, NHWC memory format has been enabled for\n  most key CPU operators, though not all of them have been merged to PyTorch\n  master branch yet. They are expected to be fully landed in PyTorch upstream\n  soon.\n* Auto Mixed Precision (AMP): Low precision data type BFloat16 has been\n  natively supported on the 3rd Generation Xeon scalable Servers (aka Cooper\n  Lake) with AVX512 instruction set and will be supported on the next\n  generation of Intel\u00ae Xeon\u00ae Scalable Processors with Intel\u00ae Advanced Matrix\n  Extensions (Intel\u00ae AMX) instruction set with further boosted performance. The\n  support of Auto Mixed Precision (AMP) with BFloat16 for CPU and BFloat16\n  optimization of operators have been massively enabled in Intel\u00ae Extension\n  for PyTorch*, and partially upstreamed to PyTorch master branch. Most of\n  these optimizations will be landed in PyTorch master through PRs that are\n  being submitted and reviewed.\n* Graph Optimization: To optimize performance further with torchscript,\n  Intel\u00ae Extension for PyTorch* supports fusion of frequently used operator\n  patterns, like Conv2D+ReLU, Linear+ReLU, etc. The benefit of the fusions are\n  delivered to users in a transparent fashion. Detailed fusion patterns\n  supported can be found `here <https://github.com/intel/intel-extension-for-pytorch>`_.\n  The graph optimization will be up-streamed to PyTorch with the introduction\n  of oneDNN Graph API.\n* Operator Optimization: Intel\u00ae Extension for PyTorch* also optimizes\n  operators and implements several customized operators for performance. A few\n  ATen operators are replaced by their optimized counterparts in Intel\u00ae\n  Extension for PyTorch* via ATen registration mechanism. Moreover, some\n  customized operators are implemented for several popular topologies. For\n  instance, ROIAlign and NMS are defined in Mask R-CNN. To improve performance\n  of these topologies, Intel\u00ae Extension for PyTorch* also optimized these\n  customized operators.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Getting Started\n---------------\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Minor code changes are required for users to get start with Intel\u00ae Extension\nfor PyTorch*. Both PyTorch imperative mode and TorchScript mode are\nsupported. This section introduces usage of Intel\u00ae Extension for PyTorch* API\nfunctions for both imperative mode and TorchScript mode, covering data type\nFloat32 and BFloat16. C++ usage will also be introduced at the end.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You just need to import Intel\u00ae Extension for PyTorch* package and apply its\noptimize function against the model object. If it is a training workload, the\noptimize function also needs to be applied against the optimizer object.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For training and inference with BFloat16 data type, torch.cpu.amp has been\nenabled in PyTorch upstream to support mixed precision with convenience, and\nBFloat16 datatype has been enabled excessively for CPU operators in PyTorch\nupstream and Intel\u00ae Extension for PyTorch*. Running torch.cpu.amp will match\neach operator to its appropriate datatype and returns the best possible\nperformance.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The code changes that are required for Intel\u00ae Extension for PyTorch* are\nhighlighted with comments in a line above.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training\n~~~~~~~~\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Float32\n^^^^^^^\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport torch.nn as nn\n# Import intel_extension_for_pytorch\nimport intel_extension_for_pytorch as ipex\n\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.linear = nn.Linear(4, 5)\n\n    def forward(self, input):\n        return self.linear(input)\n\nmodel = Model()\nmodel.set_state_dict(torch.load(PATH))\noptimizer.set_state_dict(torch.load(PATH))\n# Invoke optimize function against the model object and optimizer object\nmodel, optimizer = ipex.optimize(model, optimizer, dtype=torch.float32)\n\nfor images, label in train_loader():\n    # Setting memory_format to torch.channels_last could improve performance with 4D input data. This is optional.\n    images = images.to(memory_format=torch.channels_last)\n    loss = criterion(model(images), label)\n    loss.backward()\n    optimizer.step()\ntorch.save(model.state_dict(), PATH)\ntorch.save(optimizer.state_dict(), PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "BFloat16\n^^^^^^^^\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport torch.nn as nn\n# Import intel_extension_for_pytorch\nimport intel_extension_for_pytorch as ipex\n\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.linear = nn.Linear(4, 5)\n\n    def forward(self, input):\n        return self.linear(input)\n\nmodel = Model()\nmodel.set_state_dict(torch.load(PATH))\noptimizer.set_state_dict(torch.load(PATH))\n# Invoke optimize function against the model object and optimizer object with data type set to torch.bfloat16\nmodel, optimizer = ipex.optimize(model, optimizer, dtype=torch.bfloat16)\n\nfor images, label in train_loader():\n    with torch.cpu.amp.autocast():\n        # Setting memory_format to torch.channels_last could improve performance with 4D input data. This is optional.\n        images = images.to(memory_format=torch.channels_last)\n        loss = criterion(model(images), label)\n    loss.backward()\n    optimizer.step()\ntorch.save(model.state_dict(), PATH)\ntorch.save(optimizer.state_dict(), PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inference - Imperative Mode\n~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Float32\n\"\"\"\"\"\"\"\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport torch.nn as nn\n# Import intel_extension_for_pytorch\nimport intel_extension_for_pytorch as ipex\n\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.linear = nn.Linear(4, 5)\n\n    def forward(self, input):\n        return self.linear(input)\n\ninput = torch.randn(2, 4)\nmodel = Model()\nmodel.eval()\n# Invoke optimize function against the model object\nmodel = ipex.optimize(model, dtype=torch.float32)\nres = model(input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "BFloat16\n^^^^^^^^\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport torch.nn as nn\n# Import intel_extension_for_pytorch\nimport intel_extension_for_pytorch as ipex\n\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.linear = nn.Linear(4, 5)\n\n    def forward(self, input):\n        return self.linear(input)\n\ninput = torch.randn(2, 4)\nmodel = Model()\nmodel.eval()\n# Invoke optimize function against the model object with data type set to torch.bfloat16\nmodel = ipex.optimize(model, dtype=torch.bfloat16)\nwith torch.cpu.amp.autocast():\n    res = model(input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inference - TorchScript Mode\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TorchScript mode makes graph optimization possible, hence improves\nperformance for some topologies. Intel\u00ae Extension for PyTorch* enables most\ncommonly used operator pattern fusion, and users can get the performance\nbenefit without additional code changes.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Float32\n\"\"\"\"\"\"\"\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport torch.nn as nn\n# Import intel_extension_for_pytorch\nimport intel_extension_for_pytorch as ipex\n\n# oneDNN graph fusion is enabled by default, uncomment the line below to disable it explicitly\n# ipex.enable_onednn_fusion(False)\n\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.linear = nn.Linear(4, 5)\n\n    def forward(self, input):\n        return self.linear(input)\n\ninput = torch.randn(2, 4)\nmodel = Model()\nmodel.eval()\n# Invoke optimize function against the model object\nmodel = ipex.optimize(model, dtype=torch.float32)\nmodel = torch.jit.trace(model, torch.randn(2, 4))\nmodel = torch.jit.freeze(model)\nres = model(input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "BFloat16\n^^^^^^^^\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport torch.nn as nn\n# Import intel_extension_for_pytorch\nimport intel_extension_for_pytorch as ipex\n\n# oneDNN graph fusion is enabled by default, uncomment the line below to disable it explicitly\n# ipex.enable_onednn_fusion(False)\n\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.linear = nn.Linear(4, 5)\n\n    def forward(self, input):\n        return self.linear(input)\n\ninput = torch.randn(2, 4)\nmodel = Model()\nmodel.eval()\n# Invoke optimize function against the model with data type set to torch.bfloat16\nmodel = ipex.optimize(model, dtype=torch.bfloat16)\nwith torch.cpu.amp.autocast():\n    model = torch.jit.trace(model, torch.randn(2, 4))\n    model = torch.jit.freeze(model)\n    res = model(input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "C++\n~~~\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To work with libtorch, C++ library of PyTorch, Intel\u00ae Extension for PyTorch*\nprovides its C++ dynamic library as well. The C++ library is supposed to handle\ninference workload only, such as service deployment. For regular development,\nplease use Python interface. Comparing to usage of libtorch, no specific code\nchanges are required, except for converting input data into channels last data\nformat. Compilation follows the recommended methodology with CMake. Detailed\ninstructions can be found in `PyTorch tutorial <https://pytorch.org/tutorials/advanced/cpp_export.html#depending-on-libtorch-and-building-the-application>`_.\nDuring compilation, Intel optimizations will be activated automatically\nonce C++ dynamic library of Intel\u00ae Extension for PyTorch* is linked.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**example-app.cpp**\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "'''\n#include <torch/script.h>\n#include <iostream>\n#include <memory>\n\nint main(int argc, const char* argv[]) {\n    torch::jit::script::Module module;\n    try {\n        module = torch::jit::load(argv[1]);\n    }\n    catch (const c10::Error& e) {\n        std::cerr << \"error loading the model\\n\";\n        return -1;\n    }\n    std::vector<torch::jit::IValue> inputs;\n    // make sure input data are converted to channels last format\n    inputs.push_back(torch::ones({1, 3, 224, 224}).to(c10::MemoryFormat::ChannelsLast));\n\n    at::Tensor output = module.forward(inputs).toTensor();\n\n    return 0;\n}\n'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**CMakeList.txt**\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "'''\ncmake_minimum_required(VERSION 3.0 FATAL_ERROR)\nproject(example-app)\n\nfind_package(Torch REQUIRED)\nset(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} ${TORCH_CXX_FLAGS} -Wl,--no-as-needed\")\n\nadd_executable(example-app example-app.cpp)\n# Link the binary against the C++ dynamic library file of Intel\u00ae Extension for PyTorch*\ntarget_link_libraries(example-app \"${TORCH_LIBRARIES}\" \"${INTEL_EXTENSION_FOR_PYTORCH_PATH}/lib/libintel-ext-pt-cpu.so\")\n\nset_property(TARGET example-app PROPERTY CXX_STANDARD 14)\n'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note:** Since Intel\u00ae Extension for PyTorch* is still under development, name of\nthe c++ dynamic library in the master branch may defer to\n*libintel-ext-pt-cpu.so* shown above. Please check the name out in the\ninstallation folder. The so file name starts with *libintel-*.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Command for compilation**\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "'''\ncmake -DCMAKE_PREFIX_PATH=<LIBPYTORCH_PATH> -DINTEL_EXTENSION_FOR_PYTORCH_PATH=<INTEL_EXTENSION_FOR_PYTORCH_INSTALLATION_PATH> ..\n'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tutorials\n---------\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Please visit `Intel\u00ae Extension for PyTorch* Github repo <https://github.com/intel/intel-extension-for-pytorch>`_ for more tutorials.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}