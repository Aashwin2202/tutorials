{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nSpeech Recognition with Torchaudio\n==================================\n\n**Author**: `Moto Hira <moto@fb.com>`__\n\nThis tutorial shows how to perform speech recognition using using\npre-trained models from wav2vec 2.0\n[`paper <https://arxiv.org/abs/2006.11477>`__].\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Overview\n--------\n\nThe process of speech recognition looks like the following.\n\n1. Extract the acoustic features from audio waveform\n\n2. Estimate the class of the acoustic features frame-by-frame\n\n3. Generate hypothesis from the sequence of the class probabilities\n\nTorchaudio provides easy access to the pre-trained weights and\nassociated information, such as the expected sample rate and class\nlabels. They are bundled together and available under\n``torchaudio.pipelines`` module.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Preparation\n-----------\n\nFirst we import the necessary packages, and fetch data that we work on.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# %matplotlib inline\n\nimport torch\nimport torchaudio\n\nprint(torch.__version__)\nprint(torchaudio.__version__)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nprint(device)\n\n\nimport matplotlib\nimport matplotlib.pyplot as plt\n\n[width, height] = matplotlib.rcParams['figure.figsize']\nif width < 10:\n  matplotlib.rcParams['figure.figsize'] = [width * 2.5, height]\n\nimport IPython\n\nimport requests\n\nSPEECH_FILE = \"speech.wav\"\n\nurl = \"https://pytorch-tutorial-assets.s3.amazonaws.com/VOiCES_devkit/source-16k/train/sp0307/Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042.wav\"\nwith open(SPEECH_FILE, 'wb') as file_:\n  file_.write(requests.get(url).content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creating a pipeline\n-------------------\n\nFirst, we will create a Wav2Vec2 model that performs the feature\nextraction and the classification.\n\nThere are two types of Wav2Vec2 pre-trained weights available in\ntorchaudio. The ones fine-tuned for ASR task, and the ones not\nfine-tuned.\n\nWav2Vec2 (and HuBERT) models are trained in self-supervised manner. They\nare firstly trained with audio only for representation learning, then\nfine-tuned for a specific task with additional labels.\n\nThe pre-trained weights without fine-tuning can be fine-tuned\nfor other downstream tasks as well, but this tutorial does not\ncover that.\n\nWe will use ``torchaudio.pipelines.WAV2VEC2_ASR_BASE_960H`` here.\n\nThere are multiple models available in\n``torchaudio.pipelines``. Please check the\n`documentation <https://pytorch.org/audio/stable/pipelines.html>`__ for\nthe detail of how they are trained.\n\nThe bundle object provides the interface to instantiate model and other\ninformation. Sampling rate and the class labels are found as follow.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "bundle = torchaudio.pipelines.WAV2VEC2_ASR_BASE_960H\n\nprint(\"Sample Rate:\", bundle.sample_rate)\n\nprint(\"Labels:\", bundle.get_labels())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Model can be constructed as following. This process will automatically\nfetch the pre-trained weights and load it into the model.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = bundle.get_model().to(device)\n\nprint(model.__class__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Loading data\n------------\n\nWe will use the speech data from `VOiCES\ndataset <https://iqtlabs.github.io/voices/>`__, which is licensed under\nCreative Commos BY 4.0.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "IPython.display.display(IPython.display.Audio(SPEECH_FILE))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To load data, we use ``torchaudio.load``.\n\nIf the sampling rate is different from what the pipeline expects, then\nwe can use ``torchaudio.functional.resample`` for resampling.\n\n**Note** -\n```torchaudio.functional.resample`` <https://pytorch.org/audio/stable/functional.html#resample>`__\nworks on CUDA tensors as well. - When performing resampling multiple\ntimes on the same set of sample rates, using\n```torchaudio.transforms.Resample`` <https://pytorch.org/audio/stable/transforms.html#resample>`__\nmight improve the performace.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "waveform, sample_rate = torchaudio.load(SPEECH_FILE)\nwaveform = waveform.to(device)\n\nif sample_rate != bundle.sample_rate:\n  waveform = torchaudio.functional.resample(waveform, sample_rate, bundle.sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Extracting acoustic features\n----------------------------\n\nThe next step is to extract acoustic features from the audio.\n\nNote that Wav2Vec2 models fine-tuned for ASR task can perform feature\nextraction and classification with one step, but for the sake of the\ntutorial, we also show how to perform feature extraction here.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "with torch.inference_mode():\n  features, _ = model.extract_features(waveform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The returned features is a list of tensors. Each tensor is the output of\na transformer layer.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for i, feats in enumerate(features):\n  plt.imshow(feats[0].cpu())\n  plt.title(f\"Feature from transformer layer {i+1}\")\n  plt.xlabel(\"Feature dimension\")\n  plt.ylabel(\"Frame (time-axis)\")\n  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Feature classification\n----------------------\n\nOnce the acoustic features are extracted, the next step is to classify\nthem into a set of categories.\n\nWav2Vec2 model provides method to perform the feature extraction and\nclassification in one step.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "with torch.inference_mode():\n  emission, _ = model(waveform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The output is in the form of logits. It is not in the form of\nprobability.\n\nLet\u2019s visualize this.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.imshow(emission[0].cpu().T)\nplt.title(\"Classification result\")\nplt.xlabel(\"Frame (time-axis)\")\nplt.ylabel(\"Class\")\nplt.colorbar()\nplt.show()\nprint(\"Class labels:\", bundle.get_labels())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that there are strong indications to certain labels across\nthe time line.\n\nNote that the class 1 to 3, (``<pad>``, ``</s>`` and ``<unk>``) have\nmostly huge negative values, this is an artifact from the original\n``fairseq`` implementation where these labels are added by default but\nnot used during the training.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generating transcripts\n----------------------\n\nFrom the sequence of label probabilities, now we want to generate\ntranscripts. The process to generate hypotheses is often called\n\u201cdecoding\u201d.\n\nDecoding is more elaborate than simple classification because\ndecoding at certain time step can be affected by surrounding\nobservations.\n\nFor example, take a word like ``night`` and ``knight``. Even if their\nprior probability distribution are differnt (in typical conversations,\n``night`` would occur way more often than ``knight``), to accurately\ngenerate transcripts with ``knight``, such as ``a knight with a sword``,\nthe decoding process has to postpone the final decision until it sees\nenough context.\n\nThere are many decoding techniques proposed, and they require external\nresources, such as word dictionary and language models.\n\nIn this tutorial, for the sake of simplicity, we will perform greeding\ndecoding which does not depend on such external components, and simply\npick up the best hypothesis at each time step. Therefore, the context\ninformation are not used, and only one transcript can be generated.\n\nWe start by defining greedy decoding algorithm.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class GreedyCTCDecoder(torch.nn.Module):\n  def __init__(self, labels, ignore):\n    super().__init__()\n    self.labels = labels\n    self.ignore = ignore\n\n  def forward(self, emission: torch.Tensor) -> str:\n    \"\"\"Given a sequence emission over labels, get the best path string\n    Args:\n      emission (Tensor): Logit tensors. Shape `[num_seq, num_label]`.\n    Returns:\n      str: The resulting transcript\n    \"\"\"\n    indices = torch.argmax(emission, dim=-1)  # [num_seq,]\n    indices = torch.unique_consecutive(indices, dim=-1)\n    indices = [i for i in indices if i not in self.ignore]\n    return ''.join([self.labels[i] for i in indices])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now create the decoder object and decode the transcript.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "decoder = GreedyCTCDecoder(\n    labels=bundle.get_labels(), \n    ignore=(0, 1, 2, 3),\n)\ntranscript = decoder(emission[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let\u2019s check the result and listen again the audio.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(transcript)\nIPython.display.display(IPython.display.Audio(SPEECH_FILE))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are few remarks in decoding.\n\nFirstly, the ASR model is fine-tuned using a loss function called CTC.\nThe detail of CTC loss is explained\n`here <https://distill.pub/2017/ctc/>`__. In CTC a blank token (\u03f5) is a\nspecial token which represents a repetition of the previous symbol. In\ndecoding, these are simply ignored.\n\nSecondly, as is explained in the feature extraction section, the\nWav2Vec2 model originated from ``fairseq`` has labels that are not used.\nThese also have to be ignored.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Conclusion\n----------\n\nIn this tutorial, we looked at how to use ``torchaudio.pipeline`` to\nperform acoustic feature extraction and speech recognition. Constructing\na model and getting the emission is as short as two lines.\n\n::\n\n   model = torchaudio.pipelines.WAV2VEC2_ASR_BASE_960H.get_model()\n   emission = model(waveforms, ...)\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}